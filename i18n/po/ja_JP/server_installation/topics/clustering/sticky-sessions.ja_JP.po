# SOME DESCRIPTIVE TITLE
# Copyright (C) YEAR Nomura Research Institute, Ltd.
# This file is distributed under the same license as the keycloak-documentation-i18n package.
# FIRST AUTHOR <EMAIL@ADDRESS>, YEAR.
# 
#, fuzzy
msgid ""
msgstr ""
"Project-Id-Version: keycloak-documentation-i18n\n"
"Last-Translator: Tsukasa Amano <t.amano@pro-japan.co.jp>, 2018\n"
"Language-Team: Japanese (Japan) (https://www.transifex.com/openstandia/teams/79437/ja_JP/)\n"
"MIME-Version: 1.0\n"
"Content-Type: text/plain; charset=UTF-8\n"
"Content-Transfer-Encoding: 8bit\n"
"Language: ja_JP\n"
"Plural-Forms: nplurals=1; plural=0;\n"

#. type: Attribute :installguide_stickysessions_name:
#, no-wrap
msgid "Sticky sessions"
msgstr "スティッキー・セッション"

#. type: Plain text
msgid ""
"Typical cluster deployment consists of the load balancer (reverse proxy) and"
" 2 or more {project_name} servers on private network. For performance "
"purposes, it may be useful if load balancer forwards all requests related to"
" particular browser session to the same {project_name} backend node."
msgstr ""
"典型的なクラスター構成は、ロードバランサー（リバース・プロキシー）とプライベート・ネットワーク上の2つ以上の{project_name}サーバーで構成されています。パフォーマンスのために、ロードバランサーが特定のブラウザー・セッションに関するリクエストをすべて同じ{project_name}バックエンド・ノードに転送することは有用かもしれません。"

#. type: Plain text
msgid ""
"The reason is, that {project_name} is using infinispan distributed cache "
"under the covers for save data related to current authentication session and"
" user session.  The Infinispan distributed caches are configured with one "
"owner by default. That means that particular session is saved just on one "
"cluster node and the other nodes need to lookup the session remotely if they"
" want to access it."
msgstr ""
"なぜなら、{project_name}が現在の認証セッションとユーザー・セッションに関するデータを保存するために、Infinispanの分散キャッシュを使用しているためです。Infinispanの分散キャッシュは、デフォルトで所有者は1つと設定されています。つまり、特定のセッションは1つのクラスター・ノードに保存され、他のノードがこのセッションにアクセスする場合は、リモートで検索する必要があります。"

#. type: Plain text
msgid ""
"For example if authentication session with ID `123` is saved in the "
"infinispan cache on `node1`, and then `node2` needs to lookup this session, "
"it needs to send the request to `node1` over the network to return the "
"particular session entity."
msgstr ""
"たとえば、ID `123` を持つ認証セッションが `node1` 上のInfinispanキャッシュに保存されていて、 `node2` "
"はこのセッションを検索する必要がある場合は、特定のセッション・エンティティーを返すためにネットワーク経由で `node1` "
"へリクエストを送信する必要があります。"

#. type: Plain text
msgid ""
"It is beneficial if particular session entity is always available locally, "
"which can be done with the help of sticky sessions.  The workflow in the "
"cluster environment with the public frontend load balancer and two backend "
"{project_name} nodes can be like this:"
msgstr ""
"特定のセッション・エンティティーが常にローカルで使用できる場合は、スティッキー・セッションに助けを借りることができます。パブリック・フロントエンド・ロードバランサーと2つのバックエンド{project_name}ノードを持つクラスター環境内のワークフローは、以下のようになります。"

#. type: Plain text
msgid "User sends initial request to see the {project_name} login screen"
msgstr "ユーザーは{project_name}のログイン画面を表示するため初回リクエストを送信します。"

#. type: Plain text
msgid ""
"This request is served by the frontend load balancer, which forwards it to "
"some random node (eg. node1). Strictly said, the node doesn't need to be "
"random, but can be chosen according to some other criterias (client IP "
"address etc). It all depends on the implementation and configuration of "
"underlying load balancer (reverse proxy)."
msgstr ""
"このリクエストは、フロントエンド・ロードバランサーによって処理され、このフロントエンド・ロードバランサーがランダムにノード（たとえばnode1）に転送します。厳密には、ノードはランダムである必要はなく、他のいくつかの基準（クライアントIPアドレスなど）によって選択することができます。それらはすべて、ロードバランサー（リバース・プロキシー）の実装と設定に依存します。"

#. type: Plain text
msgid ""
"{project_name} creates authentication session with random ID (eg. 123) and "
"saves it to the Infinispan cache."
msgstr "{project_name}は認証セッションを任意のID（たとえば123）で作成し、Infinispanキャッシュに保存します。"

#. type: Plain text
msgid ""
"Infinispan distributed cache assigns the primary owner of the session based "
"on the hash of session ID.  See "
"link:http://infinispan.org/docs/8.2.x/user_guide/user_guide.html#distribution_mode[Infinispan"
" documentation] for more details around this.  Let's assume that infinispan "
"assigned `node2` to be the owner of this session."
msgstr ""
"Infinispanの分散キャッシュは、セッションIDのハッシュに基づいてセッションの主な所有者を割り当てます。これに関する詳細は、link:http://infinispan.org/docs/8.2.x/user_guide/user_guide.html#distribution_mode[Infinispan"
" documentation]を参照してください。Infinispanがこのセッションの所有者として `node2` を割り当てたとしましょう。"

#. type: Plain text
msgid ""
"{project_name} creates the cookie `AUTH_SESSION_ID` with the format like "
"`<session-id>.<owner-node-id>` . In our example case, it will be `123.node2`"
" ."
msgstr ""
"{project_name}は `<session-id>.<owner-node-id>` のような形式のCookie "
"`AUTH_SESSION_ID` を作成します．サンプルとしては、 `123.node2` となります。"

#. type: Plain text
msgid ""
"Response is returned to the user with the {project_name} login screen and "
"the AUTH_SESSION_ID cookie in the browser"
msgstr ""
"レスポンスは、{project_name}のログイン画面とブラウザーの`AUTH_SESSION_ID` Cookieでユーザーに返されます。"

#. type: Plain text
msgid ""
"From this point, it is beneficial if load balancer forwards all the next "
"requests to the `node2` as this is the node, who is owner of the "
"authentication session with ID `123` and hence Infinispan can lookup this "
"session locally. After authentication is finished, the authentication "
"session is converted to user session, which will be also saved on `node2` "
"because it has same ID `123` ."
msgstr ""
"以上の点から、ID `123` "
"を持つ認証セッションの所有者はこのノードであり、それゆえにInfinispanはローカルでこのセッションを検索できるため、ロードバランサーが "
"`node2` に対して以降のリクエストすべてを転送するなら有益なのです。認証セッションは、認証されるとユーザー・セッションに変換され、同じID "
"`123` を持っているため `node2` に保存されます。"

#. type: Plain text
msgid ""
"The sticky session is not mandatory for the cluster setup, however it is "
"good for performance for the reasons mentioned above. You need to configure "
"your loadbalancer to sticky over the `AUTH_SESSION_ID` cookie. How exactly "
"do this is dependent on your loadbalancer."
msgstr ""
"スティッキー・セッションはクラスターの設定のために必須ではありませんが、上記の理由からパフォーマンスをあげるために有効です。 "
"`AUTH_SESSION_ID` Cookieで固定するようロードバランサーを設定する必要があります。どのように行うかはロードバランサーによります。"

#. type: Plain text
msgid ""
"It is recommended on the {project_name} side to use the system property "
"`jboss.node.name` during startup, with the value corresponding to the name "
"of your route. For example, `-Djboss.node.name=node1` will use `node1` to "
"identify the route. This route will be used by Infinispan caches and will be"
" attached to the AUTH_SESSION_ID cookie when the node is the owner of the "
"particular key. An example of the start up command with this system property"
" can be seen in <<_example-setup-with-mod-cluster,Mod Cluster Example>>."
msgstr ""
"{project_name}側では、起動時にシステム・プロパティー `jboss.node.name` "
"を使用し、経路名に対応する値を使用することをお勧めします。たとえば、 `-Djboss.node.name=node1` は経路を識別するために "
"`node1` "
"を使います。この経路はInfinispanのキャッシュで使用され、ノードが特定のキーの所有者である場合は、`AUTH_SESSION_ID` "
"Cookieに関連付けられます。このシステム・プロパティーを使用した起動コマンドの例は、<<_example-setup-with-mod-"
"cluster,Mod Clusterの例>>を参照してください。"

#. type: Plain text
msgid ""
"Typically, the route name be the same name as your backend host, but it is "
"not necessary. You can use a different route name, for example if you want "
"to hide the host name of your {project_name} server inside your private "
"network."
msgstr ""
"通常、経路名はバックエンド・ホストと同じ名前である必要はありません。たとえば、プライベート・ネットワーク内の{project_name}サーバーのホスト名を隠す場合などに、別の経路名を使用できます。"

#. type: Title ====
#, no-wrap
msgid "Disable adding the route"
msgstr "経路の追加を無効にする"

#. type: Plain text
msgid ""
"Some load balancers can be configured to add the route information by "
"themselves instead of relying on the back end {project_name} node.  However,"
" as described above, adding the route by the {project_name} is recommended. "
"This is because when done this way performance improves, since "
"{project_name} is aware of the entity that is the owner of particular "
"session and can route to that node, which is not necessarily the local node."
msgstr ""
"いくつかのロードバランサーは、バックエンドの{project_name}ノードに頼るのではなく、経路情報を自身で追加するように設定できます。ただし、前述のとおり、{project_name}による経路の追加が推奨されます。これは、{project_name}が特定のセッションの所有者であり、必ずしもローカルノードではないノードにルーティングできるエンティティーを認識しているため、このようにパフォーマンスが向上するためです。"

#. type: Plain text
msgid ""
"You are permitted to disable adding route information to the AUTH_SESSION_ID"
" cookie by {project_name}, if you prefer, by adding the following into your "
"`RHSSO_HOME/standalone/configuration/standalone-ha.xml` file in the "
"{project_name} subsystem configuration:"
msgstr ""
"必要に応じて、{project_name}サブシステム設定の `RHSSO_HOME/standalone/configuration"
"/standalone-ha.xml` ファイルに以下を追加することにより、`AUTH_SESSION_ID` "
"Cookieへの経路情報の追加を無効にすることができます。"

#. type: delimited block -
#, no-wrap
msgid ""
"<subsystem xmlns=\"urn:jboss:domain:keycloak-server:1.1\">\n"
"  ...\n"
"    <spi name=\"stickySessionEncoder\">\n"
"        <provider name=\"infinispan\" enabled=\"true\">\n"
"            <properties>\n"
"                <property name=\"shouldAttachRoute\" value=\"false\"/>\n"
"            </properties>\n"
"        </provider>\n"
"    </spi>\n"
msgstr ""
"<subsystem xmlns=\"urn:jboss:domain:keycloak-server:1.1\">\n"
"  ...\n"
"    <spi name=\"stickySessionEncoder\">\n"
"        <provider name=\"infinispan\" enabled=\"true\">\n"
"            <properties>\n"
"                <property name=\"shouldAttachRoute\" value=\"false\"/>\n"
"            </properties>\n"
"        </provider>\n"
"    </spi>\n"

#. type: delimited block -
#, no-wrap
msgid "</subsystem>\n"
msgstr "</subsystem>\n"

#. type: Title ====
#, no-wrap
msgid "Example cluster setup with mod_cluster"
msgstr "mod_clusterでのクラスター設定のサンプル"

#. type: Plain text
msgid ""
"In the example, we will use link:http://mod-cluster.jboss.org/[Mod Cluster] "
"as load balancer. One of the key features of mod cluster is, that there is "
"not much configuration on the load balancer side. Instead it requires "
"support on the backend node side. Backend nodes communicate with the load "
"balancer through the dedicated protocol called MCMP and they notify "
"loadbalancer about various events (eg. node joined or left cluster, new "
"application was deployed etc)."
msgstr ""
"サンプルとして、link:http://mod-cluster.jboss.org/[Mod Cluster]をロードバランサーとして使用します。mod"
" "
"clusterの重要な特徴の1つは、ロードバランサー側の設定が少ないことです。その代わりに、バックエンド・ノード側でのサポートが必要です。バックエンド・ノードは、MCMPと呼ばれる専用プロトコルを通じてロードバランサーと通信し、さまざまなイベント（たとえばノードの参加、またはクラスターの置換、新しいアプリケーションのデプロイなど）についてロードバランサーに通知します。"

#. type: Plain text
msgid ""
"Example setup will consist of the one {appserver_name} {appserver_version} "
"load balancer node and two {project_name} nodes."
msgstr ""
"サンプルの設定は、{appserver_name} "
"{appserver_version}ロードバランサー・ノードと2つの{project_name}ノードで構成されます。"

#. type: Plain text
msgid ""
"Clustering example require MULTICAST to be enabled on machine's loopback "
"network interface. This can be done by running the following commands under "
"root privileges (on linux):"
msgstr ""
"クラスターリングのサンプルでは、MULTICASTをマシンのループバック・ネットワーク・インターフェイスで有効にすることを要求されます。これは、root特権（Linux上）で以下のコマンドを実行して行うことができます。"

#. type: delimited block -
#, no-wrap
msgid ""
"route add -net 224.0.0.0 netmask 240.0.0.0 dev lo\n"
"ifconfig lo multicast\n"
msgstr ""
"route add -net 224.0.0.0 netmask 240.0.0.0 dev lo\n"
"ifconfig lo multicast\n"

#. type: Title =====
#, no-wrap
msgid "Load Balancer Configuration"
msgstr "ロードバランサー設定"

#. type: Plain text
msgid ""
"Unzip the {appserver_name} {appserver_version} server somewhere. Assumption "
"is location `EAP_LB`"
msgstr ""
"{appserver_name} {appserver_version}サーバーをどこかに解凍します。サンプルの場所は `EAP_LB` です。"

#. type: Plain text
msgid ""
"Edit `EAP_LB/standalone/configuration/standalone.xml` file. In the undertow "
"subsystem add the mod_cluster configuration under filters like this:"
msgstr ""
"`EAP_LB/standalone/configuration/standalone.xml` "
"ファイルを編集します。undertowサブシステム内で以下のように、filtersの下にmod_cluster設定を追加します。"

#. type: delimited block -
#, no-wrap
msgid ""
"<subsystem xmlns=\"urn:jboss:domain:undertow:4.0\">\n"
" ...\n"
" <filters>\n"
"  ...\n"
"  <mod-cluster name=\"modcluster\" advertise-socket-binding=\"modcluster\"\n"
"      advertise-frequency=\"${modcluster.advertise-frequency:2000}\"\n"
"      management-socket-binding=\"http\" enable-http2=\"true\"/>\n"
" </filters>\n"
msgstr ""
"<subsystem xmlns=\"urn:jboss:domain:undertow:4.0\">\n"
" ...\n"
" <filters>\n"
"  ...\n"
"  <mod-cluster name=\"modcluster\" advertise-socket-binding=\"modcluster\"\n"
"      advertise-frequency=\"${modcluster.advertise-frequency:2000}\"\n"
"      management-socket-binding=\"http\" enable-http2=\"true\"/>\n"
" </filters>\n"

#. type: Plain text
msgid "and `filter-ref` under `default-host` like this:"
msgstr "次に、以下のように `default-host` の下に `filter-ref` を追加します。"

#. type: delimited block -
#, no-wrap
msgid ""
"<host name=\"default-host\" alias=\"localhost\">\n"
"    ...\n"
"    <filter-ref name=\"modcluster\"/>\n"
"</host>\n"
msgstr ""
"<host name=\"default-host\" alias=\"localhost\">\n"
"    ...\n"
"    <filter-ref name=\"modcluster\"/>\n"
"</host>\n"

#. type: Plain text
msgid "Then under `socket-binding-group` add this group:"
msgstr "`socket-binding-group` の下に以下のグループを追加します。"

#. type: delimited block -
#, no-wrap
msgid ""
"<socket-binding name=\"modcluster\" port=\"0\"\n"
"    multicast-address=\"${jboss.modcluster.multicast.address:224.0.1.105}\"\n"
"    multicast-port=\"23364\"/>\n"
msgstr ""
"<socket-binding name=\"modcluster\" port=\"0\"\n"
"    multicast-address=\"${jboss.modcluster.multicast.address:224.0.1.105}\"\n"
"    multicast-port=\"23364\"/>\n"

#. type: Plain text
msgid "Save the file and run the server:"
msgstr "ファイルを保存して以下のようにサーバーを実行します。"

#. type: delimited block -
#, no-wrap
msgid ""
"cd $WILDFLY_LB/bin\n"
"./standalone.sh\n"
msgstr ""
"cd $WILDFLY_LB/bin\n"
"./standalone.sh\n"

#. type: Title =====
#, no-wrap
msgid "Backend node configuration"
msgstr "バックエンド・ノードの設定"

#. type: Plain text
msgid ""
"Unzip the {project_name} server distribution to some location. Assuming "
"location is `RHSSO_NODE1` ."
msgstr "{project_name}サーバー配布物をどこかに解凍します。サンプルの場所は `RHSSO_NODE1` です。"

#. type: Plain text
msgid ""
"Edit `RHSSO_NODE1/standalone/configuration/standalone-ha.xml` and configure "
"datasource against the shared database.  See <<_rdbms-setup-checklist, "
"Database chapter >> for more details."
msgstr ""
"共有データベースに対して `RHSSO_NODE1/standalone/configuration/standalone-ha.xml` "
"を編集してデータソースを設定します。詳しくは、<<_rdbms-setup-checklist, データベースの章>>を参照してください。"

#. type: Plain text
msgid ""
"In the undertow subsystem, add the `session-config` under the `servlet-"
"container` element:"
msgstr "undertowサブシステム内で、 `servlet-container` 要素の下に `session-config` を追加します。"

#. type: delimited block -
#, no-wrap
msgid ""
"<servlet-container name=\"default\">\n"
"    <session-cookie name=\"AUTH_SESSION_ID\" http-only=\"true\" />\n"
"    ...\n"
"</servlet-container>\n"
msgstr ""
"<servlet-container name=\"default\">\n"
"    <session-cookie name=\"AUTH_SESSION_ID\" http-only=\"true\" />\n"
"    ...\n"
"</servlet-container>\n"

#. type: Plain text
msgid ""
"Then you can configure `proxy-address-forwarding` as described in the "
"chapter <<_setting-up-a-load-balancer-or-proxy, Load Balancer >> .  Note "
"that mod_cluster uses AJP connector by default, so you need to configure "
"that one."
msgstr ""
"これで次に、<<_setting-up-a-load-balancer-or-proxy, ロードバランサー>>の章で説明したとおり、 `proxy-"
"address-forwarding` "
"を設定することができます。この設定は、mod_clusterがデフォルトでAJP接続を使用するため必要だということに注意してください。"

#. type: Plain text
msgid "That's all as mod_cluster is already configured."
msgstr "mod_clusterはすでに設定済みなので、以上です。"

#. type: Plain text
msgid ""
"The node name of the {project_name} can be detected automatically based on "
"the hostname of current server. However for more fine grained control, it is"
" recommended to use system property `jboss.node.name` to specify the node "
"name directly. It is especially useful in case that you test with 2 backend "
"nodes on same physical server etc. So you can run the startup command like "
"this:"
msgstr ""
"{project_name}のノード名は、現在のサーバーのホスト名に基づき自動的に検出されます。しかし、より細かく制御するには、システム・プロパティー "
"`jboss.node.name` "
"を使用してノード名を直接指定することをお勧めします。これは、同じ物理サーバー上で2つのバックエンド・ノードをテストする場合などに特に便利です。以下のようにstartupコマンドを実行することができます。"

#. type: delimited block -
#, no-wrap
msgid ""
"cd $RHSSO_NODE1\n"
"./standalone.sh -c standalone-ha.xml -Djboss.socket.binding.port-offset=100 -Djboss.node.name=node1\n"
msgstr ""
"cd $RHSSO_NODE1\n"
"./standalone.sh -c standalone-ha.xml -Djboss.socket.binding.port-offset=100 -Djboss.node.name=node1\n"

#. type: Plain text
msgid ""
"Configure the second backend server in same way and run with different port "
"offset and node name."
msgstr "2番目のバックエンド・サーバーを同じ方法で設定し、異なるポート・オフセットとノード名で実行します。"

#. type: delimited block -
#, no-wrap
msgid ""
"cd $RHSSO_NODE2\n"
"./standalone.sh -c standalone-ha.xml -Djboss.socket.binding.port-offset=200 -Djboss.node.name=node2\n"
msgstr ""
"cd $RHSSO_NODE2\n"
"./standalone.sh -c standalone-ha.xml -Djboss.socket.binding.port-offset=200 -Djboss.node.name=node2\n"

#. type: Plain text
msgid ""
"Access the server on `http://localhost:8080/auth` . Creation of admin user "
"is possible just from local address and without load balancer (proxy) "
"access, so you first need to access backend node directly on "
"`http://localhost:8180/auth` to create admin user."
msgstr ""
"`http://localhost:8080/auth` "
"でサーバーにアクセスします。管理者ユーザーの作成は、ロードバランサー（プロキシー）へアクセスせずに、ローカルアドレスからのみ可能です。そのため、最初に "
"`http://localhost:8180/auth` のバックエンド・ノードに直接アクセスし、管理ユーザーを作成する必要があります。"
