msgid ""
msgstr ""
"Project-Id-Version: keycloak-documentation-i18n\n"
"POT-Creation-Date: 2017-09-21 04:04+0000\n"
"PO-Revision-Date: 2017-09-21 00:26-0400\n"
"Last-Translator: wadahiro <wadahiro@gmail.com>\n"
"Language-Team: Japanese\n"
"Language: ja_JP\n"
"MIME-Version: 1.0\n"
"Content-Type: text/plain; charset=UTF-8\n"
"Content-Transfer-Encoding: 8bit\n"
"Plural-Forms: nplurals=1; plural=0;\n"
"X-Generator: crowdin.com\n"
"X-Crowdin-Project: keycloak-documentation-i18n\n"
"X-Crowdin-Language: ja\n"
"X-Crowdin-File: /develop/i18n/po/ja_JP/server_installation__topics__clustering__sticky-sessions.ja_JP.po\n"

#. type: Title ===
#, no-wrap
msgid "Sticky sessions"
msgstr ""

#. type: Plain text
msgid "Typical cluster deployment consists of the load balancer (reverse proxy) and 2 or more {project_name} servers on private network. For performance purposes, it may be useful if load balancer forwards all requests related to particular browser session to the same {project_name} backend node."
msgstr ""

#. type: Plain text
msgid "The reason is, that {project_name} is using infinispan distributed cache under the covers for save data related to current authentication session and user session.  The Infinispan distributed caches are configured with one owner by default. That means that particular session is saved just on one cluster node and the other nodes need to lookup the session remotely if they want to access it."
msgstr ""

#. type: Plain text
msgid "For example if authentication session with ID `123` is saved in the infinispan cache on `node1`, and then `node2` needs to lookup this session, it needs to send the request to `node1` over the network to return the particular session entity."
msgstr ""

#. type: Plain text
msgid "It is beneficial if particular session entity is always available locally, which can be done with the help of sticky sessions.  The workflow in the cluster environment with the public frontend load balancer and two backend {project_name} nodes can be like this:"
msgstr ""

#. type: Plain text
msgid "User sends initial request to see the {project_name} login screen"
msgstr ""

#. type: Plain text
msgid "This request is served by the frontend load balancer, which forwards it to some random node (eg. node1). Strictly said, the node doesn't need to be random, but can be chosen according to some other criterias (client IP address etc). It all depends on the implementation and configuration of underlying load balancer (reverse proxy)."
msgstr ""

#. type: Plain text
msgid "{project_name} creates authentication session with random ID (eg. 123) and saves it to the Infinispan cache."
msgstr ""

#. type: Plain text
msgid "Infinispan distributed cache assigns the primary owner of the session based on the hash of session ID.  See link:http://infinispan.org/docs/8.2.x/user_guide/user_guide.html#distribution_mode[Infinispan documentation] for more details around this.  Let's assume that infinispan assigned `node2` to be the owner of this session."
msgstr ""

#. type: Plain text
msgid "{project_name} creates the cookie `AUTH_SESSION_ID` with the format like `<session-id>.<owner-node-id>` . In our example case, it will be `123.node2` ."
msgstr ""

#. type: Plain text
msgid "Response is returned to the user with the {project_name} login screen and the AUTH_SESSION_ID cookie in the browser"
msgstr ""

#. type: Plain text
msgid "From this point, it is beneficial if load balancer forwards all the next requests to the `node2` as this is the node, who is owner of the authentication session with ID `123` and hence Infinispan can lookup this session locally. After authentication is finished, the authentication session is converted to user session, which will be also saved on `node2` because it has same ID `123` ."
msgstr ""

#. type: Plain text
msgid "The sticky session is not mandatory for the cluster setup, however it is good for performance for the reasons mentioned above. You need to configure your loadbalancer to sticky over the `AUTH_SESSION_ID` cookie. How exactly do this is dependent on your loadbalancer."
msgstr ""

#. type: Plain text
msgid "Some loadbalancers can be configured to add the route information by themselves instead of rely on the backend {project_name} node. However adding the route by the {project_name} is more clever as {project_name} knows, who is the owner of particular session entity and can route it to that node, which is not necessarily the local node.  We plan to support the setup where the route info won't be added to AUTH_SESSION_ID cookie by the {project_name} server, but instead by the load balancer. However adding the cookie by {project_name} would be still the preferred option."
msgstr ""

#. type: Title ====
#, no-wrap
msgid "Example cluster setup with mod_cluster"
msgstr ""

#. type: Plain text
msgid "In the example, we will use link:http://mod-cluster.jboss.org/[Mod Cluster] as load balancer. One of the key features of mod cluster is, that there is not much configuration on the load balancer side. Instead it requires support on the backend node side. Backend nodes communicate with the load balancer through the dedicated protocol called MCMP and they notify loadbalancer about various events (eg. node joined or left cluster, new application was deployed etc)."
msgstr ""

#. type: Plain text
msgid "Example setup will consist of the one {appserver_name} {appserver_version} load balancer node and two {project_name} nodes."
msgstr ""

#. type: Plain text
msgid "Clustering example require MULTICAST to be enabled on machine's loopback network interface. This can be done by running the following commands under root privileges (on linux):"
msgstr ""

#. type: delimited block -
#, no-wrap
msgid "route add -net 224.0.0.0 netmask 240.0.0.0 dev lo\n"
"ifconfig lo multicast\n"
msgstr ""

#. type: Title =====
#, no-wrap
msgid "Load Balancer Configuration"
msgstr ""

#. type: Plain text
msgid "Unzip the {appserver_name} {appserver_version} server somewhere. Assumption is location `EAP_LB`"
msgstr ""

#. type: Plain text
msgid "Edit `EAP_LB/standalone/configuration/standalone.xml` file. In the undertow subsystem add the mod_cluster configuration under filters like this:"
msgstr ""

#. type: delimited block -
#, no-wrap
msgid "<subsystem xmlns=\"urn:jboss:domain:undertow:4.0\">\n"
" ...\n"
" <filters>\n"
"  ...\n"
"  <mod-cluster name=\"modcluster\" advertise-socket-binding=\"modcluster\"\n"
"      advertise-frequency=\"${modcluster.advertise-frequency:2000}\"\n"
"      management-socket-binding=\"http\" enable-http2=\"true\"/>\n"
" </filters>\n"
msgstr ""

#. type: Plain text
msgid "and `filter-ref` under `default-host` like this:"
msgstr ""

#. type: delimited block -
#, no-wrap
msgid "<host name=\"default-host\" alias=\"localhost\">\n"
"    ...\n"
"    <filter-ref name=\"modcluster\"/>\n"
"</host>\n"
msgstr ""

#. type: Plain text
msgid "Then under `socket-binding-group` add this group:"
msgstr ""

#. type: delimited block -
#, no-wrap
msgid "<socket-binding name=\"modcluster\" port=\"0\"\n"
"    multicast-address=\"${jboss.modcluster.multicast.address:224.0.1.105}\"\n"
"    multicast-port=\"23364\"/>\n"
msgstr ""

#. type: Plain text
msgid "Save the file and run the server:"
msgstr ""

#. type: delimited block -
#| msgid "$ .../bin/standalone.sh\n"
msgid "cd $WILDFLY_LB/bin\n"
"./standalone.sh\n"
msgstr ""

#. type: Title =====
#, no-wrap
msgid "Backend node configuration"
msgstr ""

#. type: Plain text
msgid "Unzip the {project_name} server distribution to some location. Assuming location is `RHSSO_NODE1` ."
msgstr ""

#. type: Plain text
msgid "Edit `RHSSO_NODE1/standalone/configuration/standalone-ha.xml` and configure datasource against the shared database.  See <<_rdbms-setup-checklist, Database chapter >> for more details."
msgstr ""

#. type: Plain text
msgid "In the undertow subsystem, add the `session-config` under the `servlet-container` element:"
msgstr ""

#. type: delimited block -
#, no-wrap
msgid "<servlet-container name=\"default\">\n"
"    <session-cookie name=\"AUTH_SESSION_ID\" http-only=\"true\" />\n"
"    ...\n"
"</servlet-container>\n"
msgstr ""

#. type: Plain text
msgid "Then you can configure `proxy-address-forwarding` as described in the chapter <<_setting-up-a-load-balancer-or-proxy, Load Balancer >> .  Note that mod_cluster uses AJP connector by default, so you need to configure that one."
msgstr ""

#. type: Plain text
msgid "That's all as mod_cluster is already configured."
msgstr ""

#. type: Plain text
msgid "The node name of the {project_name} can be detected automatically based on the hostname of current server. However for more fine grained control, it is recommended to use system property `jboss.node.name` to specify the node name directly. It is especially useful in case that you test with 2 backend nodes on same physical server etc. So you can run the startup command like this:"
msgstr ""

#. type: delimited block -
#| msgid "$ .../bin/standalone.sh -Djboss.socket.binding.port-offset=100\n"
msgid "cd $RHSSO_NODE1\n"
"./standalone.sh -c standalone-ha.xml -Djboss.socket.binding.port-offset=100 -Djboss.node.name=node1\n"
msgstr ""

#. type: Plain text
msgid "Configure the second backend server in same way and run with different port offset and node name."
msgstr ""

#. type: delimited block -
#| msgid "$ .../bin/standalone.sh -Djboss.socket.binding.port-offset=100\n"
msgid "cd $RHSSO_NODE2\n"
"./standalone.sh -c standalone-ha.xml -Djboss.socket.binding.port-offset=200 -Djboss.node.name=node2\n"
msgstr ""

#. type: Plain text
msgid "Access the server on `http://localhost:8080/auth` . Creation of admin user is possible just from local address and without load balancer (proxy) access, so you first need to access backend node directly on `http://localhost:8180/auth` to create admin user."
msgstr ""

